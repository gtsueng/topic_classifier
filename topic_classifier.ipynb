{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the TopicCategory classifier for outbreak.info publications. \n",
    "\n",
    "It has not yet been tested for non-publications as it is trained completely on publication data primarily from LitCovid. It contains functions for the following tasks:\n",
    "\n",
    "1. Retrieve and format LitCovid Topics (must be done frequently)\n",
    "2. Build Behavioral and offtopic training sets (must be done frequently)\n",
    "3. Generate Models (should be done rarely, or only on newly introduced topics)\n",
    "4. Classify non-litcovid publications using models (must be done for new publications)\n",
    "5. Merge formatted LitCovid Topics and Offtopic data with predicted data for inclusion into all publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update Litcovid topics\n",
    "import os\n",
    "import pathlib\n",
    "from src.fetch_litcovid_topics import *\n",
    "    \n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "\n",
    "get_litcovid_topics(DATAPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  Update Offtopics\n",
    "import os\n",
    "import pathlib\n",
    "from src.fetch_offtopics import *\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "\n",
    "\n",
    "get_other_topics(DATAPATH,RESULTSPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching the abstracts:  2021-06-14 09:53:39.954352\n",
      "fetching complete:  0:21:01.342979\n",
      "now testing:  Behavioral Research 2021-06-14 10:14:41.315312\n",
      "now testing:  Case Descriptions 2021-06-14 11:45:47.309046\n",
      "now testing:  Clinical 2021-06-14 12:00:02.667610\n",
      "now testing:  Diagnosis 2021-06-14 12:04:02.873544\n",
      "now testing:  Environment 2021-06-14 13:50:17.173233\n",
      "now testing:  Epidemiology 2021-06-14 13:57:53.682438\n",
      "now testing:  Forecasting 2021-06-14 14:33:18.103157\n",
      "now testing:  Information Sciences 2021-06-14 14:34:53.918354\n",
      "now testing:  Mechanism 2021-06-14 14:43:30.381435\n",
      "now testing:  Prevention 2021-06-14 15:29:57.509145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtsueng\\Anaconda3\\envs\\outbreak\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now testing:  Risk Factors 2021-06-14 20:51:54.454521\n",
      "now testing:  Transmission 2021-06-14 20:56:37.818074\n",
      "now testing:  Treatment 2021-06-14 21:05:34.288737\n"
     ]
    }
   ],
   "source": [
    "#### Run classifier tests\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from src.train_classifier import *\n",
    "\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "\n",
    "testresultsdf = run_test(RESULTSPATH,topicsdf,classifierset_type='best',export_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update all models\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from src.train_classifier import *\n",
    "\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()    \n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "generate_models(MODELPATH,topicsdf,classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update single topic model\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from src.train_classifier import *\n",
    "\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()    \n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "topic_to_check = input(\"enter the topic Category: \")\n",
    "generate_models(MODELPATH,topicsdf,classifiers,topic_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update annotations for all pubs\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from src.classify_pubs import *\n",
    "from src.common import load_classifiers\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "load_annotations(MODELPATH,PREDICTPATH,RESULTSPATH,topicsdf,classifiers,newonly = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update annotations for new pubs only\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from src.classify_pubs import *\n",
    "from src.common import load_classifiers\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "load_annotations(MODELPATH,PREDICTPATH,RESULTSPATH,topicsdf,classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update (refresh) the annotations for all publications\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from src.classify_pubs import *\n",
    "from src.common import load_classifiers\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "load_annotations(MODELPATH,PREDICTPATH,RESULTSPATH,topicsdf,classifiers,newonly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the efficiency of common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   _id                        topicCategory\n",
      "0  ACTRN12618001845224                         [Prevention]\n",
      "1  ACTRN12619001134112  [Individual Prevention, Prevention]\n",
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Increasing the efficiency of the cleanresults function\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "subtopicsfile = os.path.join(DATAPATH,'subtopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "subtopicsdf = read_csv(subtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "subtopic_results = read_csv(os.path.join(RESULTSPATH,'subtopicCats.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf,subtopicsdf,subtopic_results),ignore_index=True)\n",
    "\n",
    "\n",
    "def clean_results(allresults):\n",
    "    cleanresults = allresults.groupby('_id')['topicCategory'].apply(list).reset_index(name='newTopicCategory')\n",
    "    cleanresults.rename(columns={'newTopicCategory':'topicCategory'},inplace=True)\n",
    "    return(cleanresults) \n",
    "\n",
    "cleanresults = clean_results(topicsdf)\n",
    "print(cleanresults.head(n=2))\n",
    "\n",
    "#### The new method takes 14.5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mclean_results\u001b[1;34m(allresults)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\outbreak\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\outbreak\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\outbreak\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\outbreak\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Increasing the efficiency of the old cleanresults function\n",
    "\n",
    "def clean_results(allresults):\n",
    "    allresults.drop_duplicates(keep=\"first\",inplace=True)\n",
    "    counts = allresults.groupby('_id').size().reset_index(name='counts')\n",
    "    duplicates = counts.loc[counts['counts']>1]\n",
    "    singles = counts.loc[counts['counts']==1]\n",
    "    dupids = duplicates['_id'].unique().tolist()\n",
    "    tmplist = []\n",
    "    for eachid in dupids:\n",
    "        catlist = allresults['topicCategory'].loc[allresults['_id']==eachid].tolist()\n",
    "        tmplist.append({'_id':eachid,'topicCategory':catlist})\n",
    "    tmpdf = pd.DataFrame(tmplist)  \n",
    "    tmpsingledf = allresults[['_id','topicCategory']].loc[allresults['_id'].isin(singles['_id'].tolist())]\n",
    "    idlist = tmpsingledf['_id'].tolist()\n",
    "    catlist = tmpsingledf['topicCategory'].tolist()\n",
    "    cattycat = [[x] for x in catlist]\n",
    "    list_of_tuples = list(zip(idlist,cattycat))\n",
    "    singledf = pd.DataFrame(list_of_tuples, columns = ['_id', 'topicCategory']) \n",
    "    cleanresults = pd.concat((tmpdf,singledf),ignore_index=True)\n",
    "    return(cleanresults)\n",
    "\n",
    "cleanresults = clean_results(topicsdf)\n",
    "print(cleanresults.head(n=2))\n",
    "\n",
    "#### The old method took a ridiculously long time! (over 30 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                _id topicCategory pos_pred_count  \\\n",
      "32986  pmid33506503     Mechanism              3   \n",
      "32987  pmid33979601     Mechanism              3   \n",
      "\n",
      "                                     pos_pred_algorithms  \n",
      "32986  ['Random Forest', 'MultinomialNB', 'Logistic R...  \n",
      "32987  ['Random Forest', 'MultinomialNB', 'Logistic R...  \n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### The old get agreement function\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from src.common import load_classifiers\n",
    "\n",
    "def get_agreement(PREDICTPATH,eachtopic,classifierlist):\n",
    "    agreement = pd.DataFrame(columns=['_id','topicCategory','pos_pred_count','pos_pred_algorithms'])\n",
    "    classresult = pd.DataFrame(columns=['_id','prediction','topicCategory','classifier'])\n",
    "    for eachclass in classifierlist:\n",
    "        tmpfile = read_csv(os.path.join(PREDICTPATH,eachtopic+\"_\"+eachclass+\".tsv\"),delimiter='\\t',header=0,index_col=0)\n",
    "        classresult = pd.concat((classresult,tmpfile),ignore_index=True)\n",
    "    posresults = classresult.loc[classresult['prediction']=='in category']\n",
    "    agreecounts = posresults.groupby('_id').size().reset_index(name='counts')\n",
    "    no_agree = posresults.loc[posresults['_id'].isin(agreecounts['_id'].loc[agreecounts['counts']==1].tolist())].copy()\n",
    "    no_agree.rename(columns={'classifier':'pos_pred_algorithms'},inplace=True)\n",
    "    no_agree['pos_pred_count']=1\n",
    "    no_agree.drop('prediction',axis=1,inplace=True)\n",
    "    perfect_agree = posresults.loc[posresults['_id'].isin(agreecounts['_id'].loc[agreecounts['counts']==len(classifierlist)].tolist())].copy()\n",
    "    perfect_agree['pos_pred_count']=len(classifierlist)\n",
    "    perfect_agree['pos_pred_algorithms']=str(classifierlist)\n",
    "    perfect_agree.drop(['prediction','classifier'],axis=1,inplace=True)\n",
    "    perfect_agree.drop_duplicates('_id',keep='first',inplace=True)\n",
    "    partialcountids = agreecounts['_id'].loc[((agreecounts['counts']>1)&\n",
    "                                          (agreecounts['counts']<len(classifierlist)))].tolist()\n",
    "    tmplist = []\n",
    "    for eachid in list(set(partialcountids)):\n",
    "        tmpdf = posresults.loc[posresults['_id']==eachid]\n",
    "        tmpdict = {'_id':eachid,'topicCategory':eachtopic,'pos_pred_count':len(tmpdf),\n",
    "                   'pos_pred_algorithms':str(tmpdf['classifier'].tolist())}\n",
    "        tmplist.append(tmpdict)\n",
    "    partial_agree = pd.DataFrame(tmplist)    \n",
    "    agreement = pd.concat((agreement,no_agree,partial_agree,perfect_agree),ignore_index=True)\n",
    "    return(agreement)\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "classifierlist = list(classifiers.keys())\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "CLINPREDICTPATH = os.path.join(PREDICTPATH,'clinpredict/')\n",
    "PUBPREDICTPATH = os.path.join(PREDICTPATH,'pubpredict/')\n",
    "agreement = get_agreement(PUBPREDICTPATH,'Mechanism',classifierlist)\n",
    "print(agreement.tail(n=2))\n",
    "\n",
    "#### A test run for the Mechanism topic took 54 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                _id topicCategory pos_pred_algorithms  pos_pred_count\n",
      "32986  zenodo.94885     Mechanism     [MultinomialNB]               1\n",
      "32987  zenodo.99366     Mechanism     [MultinomialNB]               1\n",
      "Wall time: 5.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### increasing the efficiency of the get agreement function\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from src.common import load_classifiers\n",
    "\n",
    "def get_agreement(PREDICTPATH,eachtopic,classifierlist):\n",
    "    classresult = pd.DataFrame(columns=['_id','prediction','topicCategory','classifier'])\n",
    "    for eachclass in classifierlist:\n",
    "        tmpfile = read_csv(os.path.join(PREDICTPATH,eachtopic+\"_\"+eachclass+\".tsv\"),delimiter='\\t',header=0,index_col=0)\n",
    "        classresult = pd.concat((classresult,tmpfile),ignore_index=True)\n",
    "    classresult.drop_duplicates(keep='first',inplace=True)\n",
    "    posresults = classresult.loc[classresult['prediction']=='in category']\n",
    "    agreement = posresults.groupby(['_id','topicCategory'])['classifier'].apply(list).reset_index(name='pos_pred_algorithms')\n",
    "    agreement['pos_pred_count'] = agreecounts['pos_pred_algorithms'].str.len()\n",
    "    return(agreement)\n",
    "\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "classifierlist = classifiers.keys()\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "CLINPREDICTPATH = os.path.join(PREDICTPATH,'clinpredict/')\n",
    "PUBPREDICTPATH = os.path.join(PREDICTPATH,'pubpredict/')\n",
    "agreement = get_agreement(PUBPREDICTPATH,'Mechanism',classifierlist)\n",
    "print(agreement.tail(n=2))\n",
    "#### A test run for Mechanism took to run 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from src.common import *\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "\n",
    "\n",
    "WIKIDATAPATH = os.path.join(DATAPATH,'from wikidata/')\n",
    "repurposetypes = ['Q12140', 'Q35456', 'Q28885102','Q8386']\n",
    "headers = {'User-Agent': 'outbreak resource topic classifier bot (https://outbreak.info/; help@outbreak.info)'}\n",
    "querystart = \"\"\"\n",
    "SELECT\n",
    "  ?item ?itemLabel ?itemAltLabel\n",
    "  ?value \n",
    "WHERE \n",
    "{\n",
    "  ?item wdt:P31 wd:\"\"\"\n",
    "queryend = \"\"\".        \n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\n",
    "\"\"\"\n",
    "repurpose = pd.DataFrame(columns=['wdid','drug_name','name','alias'])\n",
    "for eachwdid in repurposetypes:\n",
    "    query = querystart+eachwdid+queryend\n",
    "    params = {'format': 'json', 'query': query, 'headers': headers}\n",
    "    r = make_request(params)\n",
    "    if r != 0:\n",
    "        data = r.json()\n",
    "        with open(os.path.join(WIKIDATAPATH,eachwdid+'.pickle'),'wb') as dumpfile:\n",
    "            pickle.dump(data,dumpfile)\n",
    "    else:\n",
    "        with open(os.path.join(WIKIDATAPATH,eachwdid+'.pickle'),'rb') as loadfile:\n",
    "            data = pickle.load(loadfile)  \n",
    "    tmpdf = parse_wikidata(data)\n",
    "    repurpose = pd.concat((repurpose,tmpdf),ignore_index=True)\n",
    "    time.sleep(1)\n",
    "repurpose.drop_duplicates(keep='first',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'head': {'vars': ['item', 'itemLabel', 'itemAltLabel', 'value']}, 'results': {'bindings': [{'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q154'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'alcoholic beverage'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'alcoholic drink'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q2845'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'cannabis'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'flower, Maria, pot, Mota, grass, herb, reefer, weed, marihuana, marijuana, Mostaza, ganja, buds, Cannabis (drug), Hierba, La Kimberly, mostaza, tokes'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q23118'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'lysergic acid diethylamide'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'LSD, Lysergic Acid Diethylamide, LSD 25, (+)-LSD, (+)-lysergic cid diethylamide, 9,10-Didehydro-N,N-diethyl-6-methylergoline-8b-carboxamide, D-LSD, D-LSD-25, D-lysergic acid dethylamide, D-lysergic acid diethylamide, D-lysergic acid n,n-diethylamide, Dextrolysergic acid diethylamide, Diethylamid kyseliny lysergove, LSD-25, Lysergamid, Lysergamide, Lysergaure diethylamid, Lysergic acid amide, Lysergic acid diethylamide-25, lysergic sourstuff diethylamide, lysergide, Lysergide, Lysergidum, n,n-diethyl-(+)-lysergamide, N,N-diethyl-(+)-lysergamide, N,N-Diethyl-6-methyl-9,10-didehydroergoline-8-carboxamide, N,n-diethyl-d-lysergamide, N,N-diethyl-D-lysergamide, N,n-diethyllysergamide, N,N-diethyllysergamide'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q60235'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'caffeine'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'propoxyphene, 1-Methyl-Theobromine, 1-methyltheobromine, 1,3,7-trimethyl-2,3,6,7-tetrahydro-1H-purine-2,6-dione, 1,3,7-trimethyl-2,6-dioxopurine, 1,3,7-Trimethyl-3,7-dihydro-1H-purine-2,6-dione, 1,3,7-trimethylpurine-2,6-dione, 1,3,7-trimethylpurine-2,6-quinone, 1,3,7-trimethylxanthine, 3,7-Dihydro-1,3,7-trimethyl-1H-purin-2,6-dion, 3,7-Dihydro-1,3,7-trimethyl-1H-purine-2,6-dione, 7-Methyl Theophylline, 7-methyltheophylline, anhydrous caffeine, caffeine, guaranine, methyltheobromine, teína, theine, anhydrous caffeine (JP15), cafeína, caféine, caffeinum, coffein, coffeinum, hycomine, koffein, lanorinal, mateína, methyltheobromide, methylxanthine theophylline, monohydrate caffeine, thein'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q80311'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'SB 699551'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'SB-699,551, SB699551'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q191924'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'D-methamphetamine'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Methamphetamine, (+)-(S)-N-alpha-Dimethylphenethylamine, (+)-(S)-P-α-dimethylphenethylamine, (AlphaS)-N,alpha-dimethylbenzeneethanamine, (S)-N,alpha-Dimethylbenzeneethanamine, (S)-N,α-dimethylbenzeneethanamine, (αS)-N,α-dimethylbenzeneethanamine, d-1-phenyl-2-methylaminopropane, d-deoxyephedrine, d-desoxyephedrine, d-N-methylamphetamine, d-phenylisopropylmethylamine, desoxyephedrine, Dextromethamphetamine, metamfetamine, Metamfetamine, Métamfétamine, Metamfetaminum, Metanfetamina, meth, Methamphetamine, methylamphetamine, Methamphetaminum, Methyl-beta-phenylisopropylamine, methyl-β-phenylisopropylamine, methylamphetamine, N-methylamphetamine, S-methamphetamine'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q207642'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Catha edulis'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': \"chat, gat, khat, Abyssinian tea, African tea, Arabian tea, Arabic khat, Bushmen's tea, jaad, jimaa, mayirungi, qaat, qat, wild tea\"}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q229962'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': '4-fluoroamphetamine'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q369648'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'tolperisone'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Mydeton, Tolperisone'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q506319'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'generic drug'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'generic medicine'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q898969'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Synthetic cannabis'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q899641'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'lean'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'barre, dirty Sprite, drank, purple drank, purple jelly, sizzurp, syrup, Texas tea, Tsikuni, wok'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q1151289'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'D-IX'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q1363784'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Erythropoietin as a doping agent'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q1812358'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'legal intoxicant'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'legal drug, legal high'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q2893248'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Regular insulin'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'neutral insulin, soluble insulin'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q3003746'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Polyvinylpolypyrrolidone'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'crospolividone, crospovidone, E1202, Polyvinyl polypyrrolidone, PVPP'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q4069596'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Armenicum'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q4327388'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Norcodeine (stereochemistry defined)'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'N-demethylcodeine, N-Norcodeine'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q4637037'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': \"4'-Methoxy-α-pyrrolidinopropiophenone\"}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'MOPP'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q4747074'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Amlodipine/benazepril'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Lotrel'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q5014894'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'CXL 1020'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q6826331'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Mexsana'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q7341121'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'robenacoxib'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Robenacoxib, Robénacoxib, Robenacoxibum'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q10395164'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Zirrê'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q11801321'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Oxi'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q13423022'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': '1-Methylamino-1-(3,4-methylenedioxyphenyl)propane'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'M-ALPHA'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q15634101'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Tabilautide'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q16893968'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'pituitrin'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q17414611'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Qat'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q18388936'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'EMA401'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': '[S]-enantiomer of EMA400, [S]-enantiomer of PD-126,055'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q20707778'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Opicinumab'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q25099393'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': '5F-AB-FUPPYCA'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'N-(1-amino-3-methyl-1-oxobutan-2-yl)-1-(5-fluoropentyl)-5-(4-fluorophenyl)-1H-pyrazole-3-carboxamide'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q25099411'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'ADAMANTYL-THPINACA'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q25101544'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Peripherally-selective drug'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q25323684'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'phenylacetylindoles'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q25339922'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Changa'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q26306642'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'PZM21'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': '(S,S)-1-[2-(dimethylamino)-3-(4-hydroxyphenyl)propyl]-3-(1-thiophen-3-ylpropan-2-yl)urea, PZM-21'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q28086552'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': '(−)-nicotine'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Nicotine polacrilex, nicotine, Nicotine, (S)-(−)-nicotine, (S)-3-(1-methylpyrrolidin-2-yl)pyridine, (S)-3-(N-methylpyrrolidin-2-yl)pyridine, (S)-Nicotine, 3-(2-(N-methylpyrrolidinyl))pyridine, 3-(N-methylpyrollidino)pyridine, L(−)-nicotine, Nicotine betadex'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q43662180'}, 'itemLabel': {'type': 'literal', 'value': 'Q43662180'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q55862712'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Sildenafil'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q56647192'}, 'itemLabel': {'type': 'literal', 'value': 'Q56647192'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q63455837'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'phototrexate'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Phototrexate'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q76980214'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'the enzyme'}}, {'item': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q105252811'}, 'itemLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Taiwan Chingguan Yihau'}, 'itemAltLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'NRICM101, RespireAid'}}]}}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
