{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the TopicCategory classifier for outbreak.info publications. \n",
    "\n",
    "It has not yet been tested for non-publications as it is trained completely on publication data primarily from LitCovid. It contains functions for the following tasks:\n",
    "\n",
    "1. Retrieve and format LitCovid Topics (must be done frequently)\n",
    "2. Build Behavioral and offtopic training sets (must be done frequently)\n",
    "3. Generate Models (should be done rarely, or only on newly introduced topics)\n",
    "4. Classify non-litcovid publications using models (must be done for new publications)\n",
    "5. Merge formatted LitCovid Topics and Offtopic data with predicted data for inclusion into all publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Update Litcovid topics\n",
    "import os\n",
    "import pathlib\n",
    "from src.fetch_litcovid_topics import *\n",
    "    \n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "\n",
    "get_litcovid_topics(DATAPATH)\n",
    "\n",
    "## Runtime: 43.7 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "####  Update Offtopics\n",
    "import os\n",
    "import pathlib\n",
    "from src.fetch_offtopics import *\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "\n",
    "get_other_topics(DATAPATH,RESULTSPATH)\n",
    "\n",
    "## Runtime: 21 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run classifier tests\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from src.train_classifier import *\n",
    "\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "\n",
    "testresultsdf = run_test(RESULTSPATH,topicsdf,classifierset_type='best',export_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update all models\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from src.train_classifier import *\n",
    "\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()    \n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "generate_models(MODELPATH,topicsdf,classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update single topic model\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from src.train_classifier import *\n",
    "\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()    \n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "topic_to_check = input(\"enter the topic Category: \")\n",
    "generate_models(MODELPATH,topicsdf,classifiers,topic_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update annotations for all pubs\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from src.classify_pubs import *\n",
    "from src.common import load_classifiers\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "load_annotations(MODELPATH,PREDICTPATH,RESULTSPATH,topicsdf,classifiers,newonly = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update annotations for new pubs only\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from src.classify_pubs import *\n",
    "from src.common import load_classifiers\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "load_annotations(MODELPATH,PREDICTPATH,RESULTSPATH,topicsdf,classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update (refresh) the annotations for all publications\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from src.classify_pubs import *\n",
    "from src.common import load_classifiers\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "load_annotations(MODELPATH,PREDICTPATH,RESULTSPATH,topicsdf,classifiers,newonly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the efficiency of common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Increasing the efficiency of the cleanresults function\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "subtopicsfile = os.path.join(DATAPATH,'subtopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "subtopicsdf = read_csv(subtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "subtopic_results = read_csv(os.path.join(RESULTSPATH,'subtopicCats.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf,subtopicsdf,subtopic_results),ignore_index=True)\n",
    "\n",
    "\n",
    "def clean_results(allresults):\n",
    "    cleanresults = allresults.groupby('_id')['topicCategory'].apply(list).reset_index(name='newTopicCategory')\n",
    "    cleanresults.rename(columns={'newTopicCategory':'topicCategory'},inplace=True)\n",
    "    return(cleanresults) \n",
    "\n",
    "cleanresults = clean_results(topicsdf)\n",
    "print(cleanresults.head(n=2))\n",
    "\n",
    "#### The new method takes 14.5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Increasing the efficiency of the old cleanresults function\n",
    "\n",
    "def clean_results(allresults):\n",
    "    allresults.drop_duplicates(keep=\"first\",inplace=True)\n",
    "    counts = allresults.groupby('_id').size().reset_index(name='counts')\n",
    "    duplicates = counts.loc[counts['counts']>1]\n",
    "    singles = counts.loc[counts['counts']==1]\n",
    "    dupids = duplicates['_id'].unique().tolist()\n",
    "    tmplist = []\n",
    "    for eachid in dupids:\n",
    "        catlist = allresults['topicCategory'].loc[allresults['_id']==eachid].tolist()\n",
    "        tmplist.append({'_id':eachid,'topicCategory':catlist})\n",
    "    tmpdf = pd.DataFrame(tmplist)  \n",
    "    tmpsingledf = allresults[['_id','topicCategory']].loc[allresults['_id'].isin(singles['_id'].tolist())]\n",
    "    idlist = tmpsingledf['_id'].tolist()\n",
    "    catlist = tmpsingledf['topicCategory'].tolist()\n",
    "    cattycat = [[x] for x in catlist]\n",
    "    list_of_tuples = list(zip(idlist,cattycat))\n",
    "    singledf = pd.DataFrame(list_of_tuples, columns = ['_id', 'topicCategory']) \n",
    "    cleanresults = pd.concat((tmpdf,singledf),ignore_index=True)\n",
    "    return(cleanresults)\n",
    "\n",
    "cleanresults = clean_results(topicsdf)\n",
    "print(cleanresults.head(n=2))\n",
    "\n",
    "#### The old method took a ridiculously long time! (over 30 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### The old get agreement function\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from src.common import load_classifiers\n",
    "\n",
    "def get_agreement(PREDICTPATH,eachtopic,classifierlist):\n",
    "    agreement = pd.DataFrame(columns=['_id','topicCategory','pos_pred_count','pos_pred_algorithms'])\n",
    "    classresult = pd.DataFrame(columns=['_id','prediction','topicCategory','classifier'])\n",
    "    for eachclass in classifierlist:\n",
    "        tmpfile = read_csv(os.path.join(PREDICTPATH,eachtopic+\"_\"+eachclass+\".tsv\"),delimiter='\\t',header=0,index_col=0)\n",
    "        classresult = pd.concat((classresult,tmpfile),ignore_index=True)\n",
    "    posresults = classresult.loc[classresult['prediction']=='in category']\n",
    "    agreecounts = posresults.groupby('_id').size().reset_index(name='counts')\n",
    "    no_agree = posresults.loc[posresults['_id'].isin(agreecounts['_id'].loc[agreecounts['counts']==1].tolist())].copy()\n",
    "    no_agree.rename(columns={'classifier':'pos_pred_algorithms'},inplace=True)\n",
    "    no_agree['pos_pred_count']=1\n",
    "    no_agree.drop('prediction',axis=1,inplace=True)\n",
    "    perfect_agree = posresults.loc[posresults['_id'].isin(agreecounts['_id'].loc[agreecounts['counts']==len(classifierlist)].tolist())].copy()\n",
    "    perfect_agree['pos_pred_count']=len(classifierlist)\n",
    "    perfect_agree['pos_pred_algorithms']=str(classifierlist)\n",
    "    perfect_agree.drop(['prediction','classifier'],axis=1,inplace=True)\n",
    "    perfect_agree.drop_duplicates('_id',keep='first',inplace=True)\n",
    "    partialcountids = agreecounts['_id'].loc[((agreecounts['counts']>1)&\n",
    "                                          (agreecounts['counts']<len(classifierlist)))].tolist()\n",
    "    tmplist = []\n",
    "    for eachid in list(set(partialcountids)):\n",
    "        tmpdf = posresults.loc[posresults['_id']==eachid]\n",
    "        tmpdict = {'_id':eachid,'topicCategory':eachtopic,'pos_pred_count':len(tmpdf),\n",
    "                   'pos_pred_algorithms':str(tmpdf['classifier'].tolist())}\n",
    "        tmplist.append(tmpdict)\n",
    "    partial_agree = pd.DataFrame(tmplist)    \n",
    "    agreement = pd.concat((agreement,no_agree,partial_agree,perfect_agree),ignore_index=True)\n",
    "    return(agreement)\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "classifierlist = list(classifiers.keys())\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "CLINPREDICTPATH = os.path.join(PREDICTPATH,'clinpredict/')\n",
    "PUBPREDICTPATH = os.path.join(PREDICTPATH,'pubpredict/')\n",
    "agreement = get_agreement(PUBPREDICTPATH,'Mechanism',classifierlist)\n",
    "print(agreement.tail(n=2))\n",
    "\n",
    "#### A test run for the Mechanism topic took 54 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### increasing the efficiency of the get agreement function\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from src.common import load_classifiers\n",
    "\n",
    "def get_agreement(PREDICTPATH,eachtopic,classifierlist):\n",
    "    classresult = pd.DataFrame(columns=['_id','prediction','topicCategory','classifier'])\n",
    "    for eachclass in classifierlist:\n",
    "        tmpfile = read_csv(os.path.join(PREDICTPATH,eachtopic+\"_\"+eachclass+\".tsv\"),delimiter='\\t',header=0,index_col=0)\n",
    "        classresult = pd.concat((classresult,tmpfile),ignore_index=True)\n",
    "    classresult.drop_duplicates(keep='first',inplace=True)\n",
    "    posresults = classresult.loc[classresult['prediction']=='in category']\n",
    "    agreement = posresults.groupby(['_id','topicCategory'])['classifier'].apply(list).reset_index(name='pos_pred_algorithms')\n",
    "    agreement['pos_pred_count'] = agreecounts['pos_pred_algorithms'].str.len()\n",
    "    return(agreement)\n",
    "\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "classifierlist = classifiers.keys()\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "CLINPREDICTPATH = os.path.join(PREDICTPATH,'clinpredict/')\n",
    "PUBPREDICTPATH = os.path.join(PREDICTPATH,'pubpredict/')\n",
    "agreement = get_agreement(PUBPREDICTPATH,'Mechanism',classifierlist)\n",
    "print(agreement.tail(n=2))\n",
    "#### A test run for Mechanism took to run 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from src.common import *\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "\n",
    "\n",
    "WIKIDATAPATH = os.path.join(DATAPATH,'from wikidata/')\n",
    "repurposetypes = ['Q12140', 'Q35456', 'Q28885102','Q8386']\n",
    "headers = {'User-Agent': 'outbreak resource topic classifier bot (https://outbreak.info/; help@outbreak.info)'}\n",
    "querystart = \"\"\"\n",
    "SELECT\n",
    "  ?item ?itemLabel ?itemAltLabel\n",
    "  ?value \n",
    "WHERE \n",
    "{\n",
    "  ?item wdt:P31 wd:\"\"\"\n",
    "queryend = \"\"\".        \n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\n",
    "\"\"\"\n",
    "repurpose = pd.DataFrame(columns=['wdid','drug_name','name','alias'])\n",
    "for eachwdid in repurposetypes:\n",
    "    query = querystart+eachwdid+queryend\n",
    "    params = {'format': 'json', 'query': query, 'headers': headers}\n",
    "    r = make_request(params)\n",
    "    if r != 0:\n",
    "        data = r.json()\n",
    "        with open(os.path.join(WIKIDATAPATH,eachwdid+'.pickle'),'wb') as dumpfile:\n",
    "            pickle.dump(data,dumpfile)\n",
    "    else:\n",
    "        with open(os.path.join(WIKIDATAPATH,eachwdid+'.pickle'),'rb') as loadfile:\n",
    "            data = pickle.load(loadfile)  \n",
    "    tmpdf = parse_wikidata(data)\n",
    "    repurpose = pd.concat((repurpose,tmpdf),ignore_index=True)\n",
    "    time.sleep(1)\n",
    "repurpose.drop_duplicates(keep='first',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
