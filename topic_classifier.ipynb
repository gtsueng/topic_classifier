{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the TopicCategory classifier for outbreak.info publications. \n",
    "\n",
    "It has not yet been tested for non-publications as it is trained completely on publication data primarily from LitCovid. It contains functions for the following tasks:\n",
    "\n",
    "1. Retrieve and format LitCovid Topics (must be done frequently)\n",
    "2. Build Behavioral and offtopic training sets (must be done frequently)\n",
    "3. Generate Models (should be done rarely, or only on newly introduced topics)\n",
    "4. Classify non-litcovid publications using models (must be done for new publications)\n",
    "5. Merge formatted LitCovid Topics and Offtopic data with predicted data for inclusion into all publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Update Litcovid topics\n",
    "import os\n",
    "import pathlib\n",
    "from src.fetch_litcovid_topics import *\n",
    "    \n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "\n",
    "get_litcovid_topics(DATAPATH)\n",
    "\n",
    "## Runtime: 43.7 seconds\n",
    "## Runtime (2022.09.07) - 1 min 22 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "####  Update Offtopics\n",
    "import os\n",
    "import pathlib\n",
    "from src.fetch_offtopics import *\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "\n",
    "get_other_topics(DATAPATH,RESULTSPATH)\n",
    "\n",
    "## Runtime: 21 min\n",
    "## Runtime (2022.09.07 - 31min 53s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Update clinical topics\n",
    "import os\n",
    "import pathlib\n",
    "from src.fetch_clinical_trials import *\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "SUBDATAPATH = os.path.join(DATAPATH,'subtopics/')\n",
    "CLINDATAPATH = os.path.join(SUBDATAPATH,'ct_topics/')\n",
    "\n",
    "update_clin_cats(DATAPATH,CLINDATAPATH)\n",
    "## Runtime (2022.09.07 - 5 min 51 s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Update subtopics topics\n",
    "import os\n",
    "import pathlib\n",
    "from src.fetch_subtopics import *\n",
    "from src.fetch_litsubtopics import *\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "SUBDATAPATH = os.path.join(DATAPATH,'subtopics/')\n",
    "\n",
    "get_sub_topics(DATAPATH,RESULTSPATH)\n",
    "## Runtime (2022.09.09 - 1h 4min 13s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       topicCategory  counts\n",
      "0  Case Descriptions   13871\n",
      "1          Diagnosis   46859\n",
      "2        Forecasting    3029\n",
      "3          Mechanism   31912\n",
      "4         Prevention   68908\n",
      "5       Transmission    6872\n",
      "6          Treatment   67460\n",
      "          topicCategory  counts\n",
      "0   Behavioral Research   53557\n",
      "1              Clinical    4263\n",
      "2           Environment    6884\n",
      "3          Epidemiology   16296\n",
      "4  Information Sciences   11513\n",
      "5          Risk Factors    5119\n",
      "                   topicCategory  counts\n",
      "0             Antibody Detection    8262\n",
      "1                      Biologics    6107\n",
      "2         Classical Epidemiology   22484\n",
      "3                   Host Factors     686\n",
      "4   Host-Intermediate Reservoirs    1248\n",
      "5         Immunological Response   11433\n",
      "6          Individual Prevention   64223\n",
      "7         Mechanism of Infection    5468\n",
      "8      Mechanism of Transmission     829\n",
      "9                   Medical Care    5804\n",
      "10        Molecular Epidemiology     441\n",
      "11           Pathology-Radiology    9224\n",
      "12     Pharmaceutical Treatments    6062\n",
      "13                     Prognosis    6239\n",
      "14   Public Health Interventions   39846\n",
      "15             Rapid Diagnostics    2300\n",
      "16                   Repurposing    5514\n",
      "17                      Symptoms   13832\n",
      "18            Testing Prevalence    5306\n",
      "19                      Vaccines   21576\n",
      "20    Viral Shedding-Persistence     419\n",
      "21               Virus Detection   10831\n",
      "22                 Virus Factors    6136\n",
      "Wall time: 600 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Generate histogram of litcovid topics, other topics, and clinical cats\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas import read_csv\n",
    "\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "\n",
    "littopics = read_csv(os.path.join(DATAPATH,'litcovidtopics.tsv'), delimiter='\\t', header=0, index_col=0)\n",
    "print(littopics.groupby('topicCategory').size().reset_index(name=\"counts\"))\n",
    "\n",
    "offtopics = read_csv(os.path.join(DATAPATH,'othertopics.tsv'), delimiter='\\t', header=0, index_col=0)\n",
    "print(offtopics.groupby('topicCategory').size().reset_index(name=\"counts\"))\n",
    "\n",
    "subtopics = read_csv(os.path.join(DATAPATH,'subtopics.tsv'), delimiter='\\t', header=0, index_col=0)\n",
    "print(subtopics.groupby('topicCategory').size().reset_index(name=\"counts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run classifier tests\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from src.train_classifier import *\n",
    "\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "\n",
    "testresultsdf = run_test(RESULTSPATH,topicsdf,classifierset_type='best',export_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update all models\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from src.train_classifier import *\n",
    "\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()    \n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "generate_models(MODELPATH,topicsdf,classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update single topic model\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from src.train_classifier import *\n",
    "\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()    \n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "topic_to_check = input(\"enter the topic Category: \")\n",
    "generate_models(MODELPATH,topicsdf,classifiers,topic_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update annotations for all pubs\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from src.classify_pubs import *\n",
    "from src.common import load_classifiers\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "load_annotations(MODELPATH,PREDICTPATH,RESULTSPATH,topicsdf,classifiers,newonly = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update annotations for new pubs only\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from src.classify_pubs import *\n",
    "from src.common import load_classifiers\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf),ignore_index=True)\n",
    "topiclist = topicsdf['topicCategory'].unique().tolist()\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "load_annotations(MODELPATH,PREDICTPATH,RESULTSPATH,topicsdf,classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Update (refresh) the annotations for all publications\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from src.classify_pubs import *\n",
    "from src.common import load_classifiers\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "#### Refresh the classification of other resources\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "subtopicsfile = os.path.join(RESULTSPATH,'subtopicCats.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "subtopicsdf = read_csv(subtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf,subtopicsdf),ignore_index=True)\n",
    "topicsdf.drop_duplicates(keep='first',inplace=True)\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "load_annotations(DATAPATH,MODELPATH,PREDICTPATH,RESULTSPATH,topicsdf,classifiers,False)\n",
    "\n",
    "##Run time: 36min 54s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run times for different parts of cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Update (refresh) the annotations for all publications\n",
    "import os\n",
    "import pathlib\n",
    "from src.classify_pubs import *\n",
    "from src.common import load_classifiers\n",
    "from src.common import topic_dict\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "classifierlist = classifiers.keys()\n",
    "CLINPREDICTPATH = os.path.join(PREDICTPATH,'clinpredict/')\n",
    "PUBPREDICTPATH = os.path.join(PREDICTPATH,'pubpredict/')\n",
    "classify_clins(DATAPATH,MODELPATH,PREDICTPATH,classifiers,topic_dict)\n",
    "clin_total_agree = merge_predictions(CLINPREDICTPATH,topic_dict,classifierlist,agreetype='perfect')\n",
    "all_ids = get_pub_ids(sourceset=\"nonlitcovid\")\n",
    "print(len(all_ids))\n",
    "\n",
    "##Run time 8min 41s for 28372 docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799430\n",
      "                   _id         topicCategory\n",
      "0  2020.01.31.20019935  Information Sciences\n",
      "1    2020.02.03.932350  Information Sciences\n",
      "Wall time: 65 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify_pubs(MODELPATH,PUBPREDICTPATH,all_ids,topic_dict,classifiers,False)\n",
    "total_agree = merge_predictions(PUBPREDICTPATH,topic_dict,classifierlist,'perfect')\n",
    "totalresults = pd.concat((total_agree,topicsdf,clin_total_agree),ignore_index=True)\n",
    "print(len(totalresults))\n",
    "print(totalresults.head(n=2))\n",
    "\n",
    "##Run time for 28372 docs: 28min 19s, resulting in 799430 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821890\n",
      "           _id topicCategory\n",
      "145231  092619     Mechanism\n",
      "54831   105437   Forecasting\n",
      "254411\n",
      "      _id                                      topicCategory\n",
      "0  092619                                        [Mechanism]\n",
      "1  105437  [Forecasting, Mechanism, Molecular Epidemiolog...\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "allresults = include_clin(totalresults)\n",
    "allresults['topicCategory'] = allresults['topicCategory'].str.replace('-','/')\n",
    "allresults.dropna(axis=0,inplace=True)\n",
    "allresults.drop_duplicates(keep='first',inplace=True)\n",
    "allresults.reset_index(drop=True)\n",
    "cleanresults = clean_results(allresults) \n",
    "\n",
    "print(len(allresults))\n",
    "print(allresults.head(n=2))\n",
    "print(len(cleanresults))\n",
    "print(cleanresults.head(n=2))\n",
    "##Run time 21.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cleanresults.to_csv(os.path.join(RESULTSPATH,'topicCats.tsv'),mode='w',sep='\\t',header=True)\n",
    "updated_results = read_csv(os.path.join(RESULTSPATH,'topicCats.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "updated_results.drop_duplicates(subset='_id',keep='last',inplace=True)\n",
    "updated_results.to_csv(os.path.join(RESULTSPATH,'topicCats.tsv'),sep='\\t',header=True)\n",
    "updated_results.to_json(os.path.join(RESULTSPATH,'topicCats.json'), orient='records')\n",
    "\n",
    "##Run time 3.14 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the efficiency of common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Increasing the efficiency of the cleanresults function\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "littopicsfile = os.path.join(DATAPATH,'litcovidtopics.tsv')\n",
    "offtopicsfile = os.path.join(DATAPATH,'othertopics.tsv')\n",
    "subtopicsfile = os.path.join(DATAPATH,'subtopics.tsv')\n",
    "littopicsdf = read_csv(littopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "offtopicsdf = read_csv(offtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "subtopicsdf = read_csv(subtopicsfile,delimiter='\\t',header=0,index_col=0)\n",
    "subtopic_results = read_csv(os.path.join(RESULTSPATH,'subtopicCats.tsv'),delimiter='\\t',header=0,index_col=0)\n",
    "topicsdf = pd.concat((littopicsdf,offtopicsdf,subtopicsdf,subtopic_results),ignore_index=True)\n",
    "\n",
    "\n",
    "def clean_results(allresults):\n",
    "    cleanresults = allresults.groupby('_id')['topicCategory'].apply(list).reset_index(name='newTopicCategory')\n",
    "    cleanresults.rename(columns={'newTopicCategory':'topicCategory'},inplace=True)\n",
    "    return(cleanresults) \n",
    "\n",
    "cleanresults = clean_results(topicsdf)\n",
    "print(cleanresults.head(n=2))\n",
    "\n",
    "#### The new method takes 14.5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### Increasing the efficiency of the old cleanresults function\n",
    "\n",
    "def clean_results(allresults):\n",
    "    allresults.drop_duplicates(keep=\"first\",inplace=True)\n",
    "    counts = allresults.groupby('_id').size().reset_index(name='counts')\n",
    "    duplicates = counts.loc[counts['counts']>1]\n",
    "    singles = counts.loc[counts['counts']==1]\n",
    "    dupids = duplicates['_id'].unique().tolist()\n",
    "    tmplist = []\n",
    "    for eachid in dupids:\n",
    "        catlist = allresults['topicCategory'].loc[allresults['_id']==eachid].tolist()\n",
    "        tmplist.append({'_id':eachid,'topicCategory':catlist})\n",
    "    tmpdf = pd.DataFrame(tmplist)  \n",
    "    tmpsingledf = allresults[['_id','topicCategory']].loc[allresults['_id'].isin(singles['_id'].tolist())]\n",
    "    idlist = tmpsingledf['_id'].tolist()\n",
    "    catlist = tmpsingledf['topicCategory'].tolist()\n",
    "    cattycat = [[x] for x in catlist]\n",
    "    list_of_tuples = list(zip(idlist,cattycat))\n",
    "    singledf = pd.DataFrame(list_of_tuples, columns = ['_id', 'topicCategory']) \n",
    "    cleanresults = pd.concat((tmpdf,singledf),ignore_index=True)\n",
    "    return(cleanresults)\n",
    "\n",
    "cleanresults = clean_results(topicsdf)\n",
    "print(cleanresults.head(n=2))\n",
    "\n",
    "#### The old method took a ridiculously long time! (over 30 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### The old get agreement function\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from src.common import load_classifiers\n",
    "\n",
    "def get_agreement(PREDICTPATH,eachtopic,classifierlist):\n",
    "    agreement = pd.DataFrame(columns=['_id','topicCategory','pos_pred_count','pos_pred_algorithms'])\n",
    "    classresult = pd.DataFrame(columns=['_id','prediction','topicCategory','classifier'])\n",
    "    for eachclass in classifierlist:\n",
    "        tmpfile = read_csv(os.path.join(PREDICTPATH,eachtopic+\"_\"+eachclass+\".tsv\"),delimiter='\\t',header=0,index_col=0)\n",
    "        classresult = pd.concat((classresult,tmpfile),ignore_index=True)\n",
    "    posresults = classresult.loc[classresult['prediction']=='in category']\n",
    "    agreecounts = posresults.groupby('_id').size().reset_index(name='counts')\n",
    "    no_agree = posresults.loc[posresults['_id'].isin(agreecounts['_id'].loc[agreecounts['counts']==1].tolist())].copy()\n",
    "    no_agree.rename(columns={'classifier':'pos_pred_algorithms'},inplace=True)\n",
    "    no_agree['pos_pred_count']=1\n",
    "    no_agree.drop('prediction',axis=1,inplace=True)\n",
    "    perfect_agree = posresults.loc[posresults['_id'].isin(agreecounts['_id'].loc[agreecounts['counts']==len(classifierlist)].tolist())].copy()\n",
    "    perfect_agree['pos_pred_count']=len(classifierlist)\n",
    "    perfect_agree['pos_pred_algorithms']=str(classifierlist)\n",
    "    perfect_agree.drop(['prediction','classifier'],axis=1,inplace=True)\n",
    "    perfect_agree.drop_duplicates('_id',keep='first',inplace=True)\n",
    "    partialcountids = agreecounts['_id'].loc[((agreecounts['counts']>1)&\n",
    "                                          (agreecounts['counts']<len(classifierlist)))].tolist()\n",
    "    tmplist = []\n",
    "    for eachid in list(set(partialcountids)):\n",
    "        tmpdf = posresults.loc[posresults['_id']==eachid]\n",
    "        tmpdict = {'_id':eachid,'topicCategory':eachtopic,'pos_pred_count':len(tmpdf),\n",
    "                   'pos_pred_algorithms':str(tmpdf['classifier'].tolist())}\n",
    "        tmplist.append(tmpdict)\n",
    "    partial_agree = pd.DataFrame(tmplist)    \n",
    "    agreement = pd.concat((agreement,no_agree,partial_agree,perfect_agree),ignore_index=True)\n",
    "    return(agreement)\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "classifierlist = list(classifiers.keys())\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "CLINPREDICTPATH = os.path.join(PREDICTPATH,'clinpredict/')\n",
    "PUBPREDICTPATH = os.path.join(PREDICTPATH,'pubpredict/')\n",
    "agreement = get_agreement(PUBPREDICTPATH,'Mechanism',classifierlist)\n",
    "print(agreement.tail(n=2))\n",
    "\n",
    "#### A test run for the Mechanism topic took 54 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#### increasing the efficiency of the get agreement function\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from src.common import load_classifiers\n",
    "\n",
    "def get_agreement(PREDICTPATH,eachtopic,classifierlist):\n",
    "    classresult = pd.DataFrame(columns=['_id','prediction','topicCategory','classifier'])\n",
    "    for eachclass in classifierlist:\n",
    "        tmpfile = read_csv(os.path.join(PREDICTPATH,eachtopic+\"_\"+eachclass+\".tsv\"),delimiter='\\t',header=0,index_col=0)\n",
    "        classresult = pd.concat((classresult,tmpfile),ignore_index=True)\n",
    "    classresult.drop_duplicates(keep='first',inplace=True)\n",
    "    posresults = classresult.loc[classresult['prediction']=='in category']\n",
    "    agreement = posresults.groupby(['_id','topicCategory'])['classifier'].apply(list).reset_index(name='pos_pred_algorithms')\n",
    "    agreement['pos_pred_count'] = agreecounts['pos_pred_algorithms'].str.len()\n",
    "    return(agreement)\n",
    "\n",
    "\n",
    "classifiers = load_classifiers('best')\n",
    "classifierlist = classifiers.keys()\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "CLINPREDICTPATH = os.path.join(PREDICTPATH,'clinpredict/')\n",
    "PUBPREDICTPATH = os.path.join(PREDICTPATH,'pubpredict/')\n",
    "agreement = get_agreement(PUBPREDICTPATH,'Mechanism',classifierlist)\n",
    "print(agreement.tail(n=2))\n",
    "#### A test run for Mechanism took to run 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from src.common import *\n",
    "#### MAIN\n",
    "#script_path = pathlib.Path(__file__).parent.absolute()\n",
    "script_path = ''\n",
    "DATAPATH = os.path.join(script_path,'data/')\n",
    "RESULTSPATH = os.path.join(script_path,'results/')\n",
    "MODELPATH = os.path.join(script_path,'models/')\n",
    "PREDICTPATH = os.path.join(script_path,'predictions/')\n",
    "\n",
    "\n",
    "\n",
    "WIKIDATAPATH = os.path.join(DATAPATH,'from wikidata/')\n",
    "repurposetypes = ['Q12140', 'Q35456', 'Q28885102','Q8386']\n",
    "headers = {'User-Agent': 'outbreak resource topic classifier bot (https://outbreak.info/; help@outbreak.info)'}\n",
    "querystart = \"\"\"\n",
    "SELECT\n",
    "  ?item ?itemLabel ?itemAltLabel\n",
    "  ?value \n",
    "WHERE \n",
    "{\n",
    "  ?item wdt:P31 wd:\"\"\"\n",
    "queryend = \"\"\".        \n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\n",
    "\"\"\"\n",
    "repurpose = pd.DataFrame(columns=['wdid','drug_name','name','alias'])\n",
    "for eachwdid in repurposetypes:\n",
    "    query = querystart+eachwdid+queryend\n",
    "    params = {'format': 'json', 'query': query, 'headers': headers}\n",
    "    r = make_request(params)\n",
    "    if r != 0:\n",
    "        data = r.json()\n",
    "        with open(os.path.join(WIKIDATAPATH,eachwdid+'.pickle'),'wb') as dumpfile:\n",
    "            pickle.dump(data,dumpfile)\n",
    "    else:\n",
    "        with open(os.path.join(WIKIDATAPATH,eachwdid+'.pickle'),'rb') as loadfile:\n",
    "            data = pickle.load(loadfile)  \n",
    "    tmpdf = parse_wikidata(data)\n",
    "    repurpose = pd.concat((repurpose,tmpdf),ignore_index=True)\n",
    "    time.sleep(1)\n",
    "repurpose.drop_duplicates(keep='first',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
